# data platform
	
## cosmos db: 
		document db, globally (elastically and independently scale throughput and storage across any regions) distributed, multi-model database.
		store data in atom-record-sequence formart at loweast level, then provide several api to abstract and project raw data: SQL, MongoDB, Cassandra, Tables, Gremlin.
## azure sql database: 
		relational database, PaaS (it handle db management: patching, upgrading, backups, monitoring for us), fully managed service with built-in high availability, security, fault tolerance, scalibility. also support json, xml.  with newest sql server features. all version/tier support config retention(daily/monthly,weekly...) up to 10 years, 
		az.
		migration: data migration assistant (tool which also can do assess before migrate) or use azure database migration service , support offline/online mode (online mode need premium sql).
		support hyperscale mode (multiple read instance), DTU, vCore mode
## Azure sql managed instance: 
		provide more compatible service then azure sql database (offer mode options)
		vCore mode
	
## Azure Mysql: 
		base on community edition db engine, 5.6+, PaaS like azure sql database. 
## Azure postgreSQL: 
		base on community edition db engine, PaaS like azure sql database. support Single Server Mode or Hyperscale(Citus) mode,
		Hyperscale(Citus): horizontally scale queries across multiple machines by using sharding, query engine parallizes imcomming SQL across these servers for fast responses on large dateset.
	
## Azure Synapse Analytics: 
		data warehouse + big data analytics, load data from file, sql, spark.., then use those data.
## HDInsight (open source analytics service)/Azure Databricks/Azure Data Lake Analytics: 
		similar as Azure Synapse Analytics
	
## Azure SQL:
		we need one logical server to host our database(we can control logins, firewalls, policies there),
		DTU(Database Transaction Unit): a combined measure of compute+storage+io resources. 
		eDTU(elastic Database Transaction Unit): for elastic pool, is server level resource, which can share between many databases in a server.
		vCores:virtual cores, allow us to config compute and storage resources seperately.
		SQL elastic pool: related to eDTUs, we can buy a set of compute and storage resources and share between all databases in the pool.
		collation: SQL_Latin1_General_CP1_CI_AS, 
				Latin1_General refers to the family of Western European languages.
				CP1 refers to code page 1252, a popular character encoding of the Latin alphabet.
				CI means that comparisons are case insensitive. For example, "HELLO" compares equally to "hello".
				AS means that comparisons are accent sensitive. For example, "résumé" doesn't compare equally to "resume".
		in command line, use sqlcmd to connect to sql, when execute sql query, run GO after each query. 
		command to create resource like elastic pool: az sql elastic-pools create / New-AzSqlElasticPool
		bcp utility can used to bulk upload data.
	data migration assistant/azure database migration service: assess incompatibilities(by azure migration assistant), migrate schema, migrate data(azure database migration service: offline, online - premium only), post migration.


## cosmos db: 
		create a cosmos db account first, select an API (SQL/mongo/etc.), create database, create container with partition key, then CURD data.
			RU: request units, cosmos measure throughput by RU, 1RU means, 1 read/write for 1kb data a second, things affect RU like, item size(operation), item indexing, item property count, indexed properties, Data consistency level(strong to staleness), query pattern(from predicates, function, result set size...), sp/trigger usage...
		
			if exceeding RU, request will be limited.
		
			partition key: enable us add more partitions to our database when needed, set when create an container and cannot be changed. define partition strategy: better can evenly distributed our operations (on data) to many partitions(horizontal scaling). the amount of required RU's and storage determines the number of partitions cosmos need, cosmos will manage that with out downtime or performance impact of our applications.
		
			a partition can have up to 20GB storage, if a single partition key can result in exceeding the space, also can use composited partition key, like userId+date, or userId+productId,etc.
		
			distribution: replicate data to multiple regions to delivering low-lantency (the closet region) data access to end users and adding regional resiliency (region pairs: 2 regions in same geographies and ensure serialized platform update across region pairs) for business continuity and disaster recovery(BCDR).
		
			throughput and storage are replicated for every regions, so we may have around 4 times cost if we replicate to 3 regions.
		
			multi-master model: multiple wirte to several regions (write to one region, then propagated to all other regions immediately, by default last-writer-win conflict resolution, but can config custom resolution like function/custom applications). 
		
			failover: for multiple write model, SDK will automatically redirect to next closest region if a regional failure happens. for Single-write, multi-read mode, we need enable the automatic failover option first,then we can change the priority of read nodes, if write nodes fails, cosmos will select a read node as new write node base on priority. once the failed node comes back,  the node will be automatic synced to latest version.
		
			Consistency level: Strong -> Bounded Staleness->Session->Consistency Prefix->Eventual
				Strong: Reads are guaranteed to return the latest item.
				Bounded Staleness: Reads lg behind for at least k versions(or prefix) or t interval.
				Session: Monotonic read/write, read you own write guarantees.
				Consistency Prefix: ensure never see out of order write, if write perform like A,B,C, it will not see A,C or A, C, B, but may see A, B, or A,B, B.
				Eventual: client may get values may old then it had seen before.
			Cosmos sql format(join here is inner join for join document with document subroots):
				SELECT <select_list>
				[FROM <optional_from_specification>]
				[WHERE <optional_filter_condition>]
				[ORDER BY <optional_sort_specification>]
				[JOIN <optional_join_specification>]
			Cosmos Stored procedures: 
				they are javascript functions with document context, they are the only way to ensure ACID transactions runs on server.
			UDFs(user-defined function) are also on server, but they dont have document context, they can only support implement some reusable calculations.
		
## Azure synapse analytics: 
		with a SQL pool using data warehouse units (DWU: bundle CPU+memory+throughput, maximum 30000), adjust the DWU and storage seperately.
	
		Massively parallel processing(Control node + cumpute nodes with Data movement service: DMS ), workload management (control resource/importance level/timeout for queries), result-set cache(cache query result in SQL pool storage), materialized views(keep an up to date pre-compute data), CI/CD use database project to track/deploy/test schema changes...

		table geometries(distribution): hash(idealy evenly distribute data, highest performance for big table), round robin(for staging table or temp table), replicate(small table)
	
		obtain source data -> update to blob storage -> get storage account access key + resource url -> open synapse query editor -> (Using PolyBase then) create an database credential using the access key -> create an datasource using the resource url + credential created ->
		
		create a file format -> create a temp table with the datasource created + file format created -> create an destination table from temp table -> query or add statistics then query.
	
		to increase upload speed, we can split the file and define parallel upload + import processes.